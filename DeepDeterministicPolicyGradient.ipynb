{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-578TXDLZF47"
   },
   "outputs": [],
   "source": [
    "# Run this to install tensorflow version 1.9\n",
    "# !pip install -q tensorflow==1.9\n",
    "# Make sure you RESTART RUNTIME (Runtime->Restart Runtime) after running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HnA-Zf8JqsL4",
    "outputId": "f1b799cb-e735-4c19-e59d-254b99c627d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "print(\"Gym:\", gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YHkYC1tmqsMB",
    "outputId": "27b9c0d6-bc0f-426c-9bdb-b791aba2a60e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-8.0, 8.0, (3,), float32)\n",
      "Action space: Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Pendulum-v0\"\n",
    "# env_name = \"MountainCarContinuous-v0\"\n",
    "# env_name = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow QNetwork\n",
    "\n",
    "Below is the implementation of an Actor Critic network using tensorflow as the backend model. This contains a tensorflow session and after initially building the network graph, it is trained by running the optimizer with the state, action for accessing the action and q values and the next_state, reward and done for calculating the q_target values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79GCjcYYqsMG"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4388fa0e6956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensorflow:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTFACNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "class TFACNetwork():\n",
    "    def __init__(self, state_size, action_size, action_range):\n",
    "        tf.reset_default_graph()\n",
    "        self.state_in = tf.placeholder(tf.float32, shape=[None, state_size])\n",
    "        self.action_in = tf.placeholder(tf.float32, shape=[None, action_size])\n",
    "        self.next_state_in = tf.placeholder(tf.float32, shape=[None, state_size])\n",
    "        self.reward_in = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.done_in = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        self.action_low, self.action_high = action_range\n",
    "        self.actor_local = self.build_actor(action_size, scope=\"actor_local\")\n",
    "        self.actor_target = self.build_actor(action_size, scope=\"actor_target\")\n",
    "        self.critic_local = self.build_critic(self.state_in, self.action_in, scope=\"critic_local\")\n",
    "        self.critic_target = self.build_critic(self.next_state_in, self.actor_target, scope=\"critic_target\")\n",
    "\n",
    "        self.optimizer = self.build_optimizer()\n",
    "        self.updater = self.build_updater()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "          \n",
    "    def build_actor(self, action_size, scope=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            layer1 = tf.layers.dense(self.state_in, 100, activation=tf.nn.relu)\n",
    "            layer2 = tf.layers.dense(layer1, 100, activation=tf.nn.relu)\n",
    "            layer3 = tf.layers.dense(layer2, 100, activation=tf.nn.relu)\n",
    "            action_raw = tf.layers.dense(layer3, action_size, activation=None)\n",
    "            action_scaled = self.action_low + (self.action_high - self.action_low) * tf.nn.sigmoid(action_raw)\n",
    "            return action_scaled\n",
    "\n",
    "    def build_critic(self, state, action, scope=None, reuse=False):\n",
    "        with tf.variable_scope(scope):\n",
    "            net_state = tf.layers.dense(state, 100, activation=tf.nn.relu, reuse=reuse, name=\"critic1\")\n",
    "            net_action = tf.layers.dense(action, 100, activation=tf.nn.relu, reuse=reuse, name=\"critic2\")\n",
    "            net_state_action = tf.concat([net_state, net_action], axis=1)\n",
    "            net_layer = tf.layers.dense(net_state_action, 100, activation=tf.nn.relu, reuse=reuse, name=\"critic3\")\n",
    "            q_value = tf.layers.dense(net_layer, 1, activation=None, reuse=reuse, name=\"critic4\")\n",
    "            return q_value\n",
    "        \n",
    "    def build_updater(self, tau=0.01):\n",
    "        actor_update_ops = [tf.assign(t, t+tau*(l-t)) for l,t in zip(self.get_vars(\"actor_local\"), self.get_vars(\"actor_target\"))]\n",
    "        critic_update_ops = [tf.assign(t, t+tau*(l-t)) for l,t in zip(self.get_vars(\"critic_local\"), self.get_vars(\"critic_target\"))]\n",
    "        updater = tf.group(*actor_update_ops, *critic_update_ops)\n",
    "        return updater\n",
    "        \n",
    "    def build_optimizer(self, learn_rate=0.00025, gamma=0.97):\n",
    "        rewards = tf.expand_dims(self.reward_in, axis=1)\n",
    "        dones = tf.expand_dims(self.done_in, axis=1)\n",
    "        q_targets = rewards + gamma * self.critic_target * (1-dones)\n",
    "        critic_error = q_targets - self.critic_local\n",
    "        critic_loss = tf.reduce_mean(tf.square(critic_error))\n",
    "        critic_optimizer = tf.train.AdamOptimizer(learn_rate).minimize(critic_loss, var_list=self.get_vars(\"critic_local\"))\n",
    "        \n",
    "        actor_trainer = self.build_critic(self.state_in, self.actor_local, scope=\"critic_local\", reuse=True)\n",
    "        actor_gain = tf.reduce_mean(actor_trainer - self.critic_local)\n",
    "        actor_optimizer = tf.train.AdamOptimizer(learn_rate).minimize(-actor_gain, var_list=self.get_vars(\"actor_local\"))\n",
    "        \n",
    "        optimizer = tf.group(actor_optimizer, critic_optimizer)\n",
    "        return optimizer\n",
    "        \n",
    "    def get_vars(self, scope):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "    \n",
    "    def update_model(self, state, action, next_state, reward, done):\n",
    "        feed = {self.state_in: state, self.action_in: action, self.next_state_in: next_state, self.reward_in: reward, self.done_in: done}\n",
    "        self.sess.run([self.optimizer, self.updater], feed_dict=feed)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.sess.run(self.actor_local, feed_dict={self.state_in: state})\n",
    "        return action\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch QNetwork\n",
    "\n",
    "Below is the implementation of an Actor Critic network using pytorch as the backend model. This involves first defining a Model class for both the Actor and Critic which subclasses the pytorch nn.Module class and then defines the network graph which can be run with the forward function.\n",
    "\n",
    "Then the Model is included in an enclosing PTQNetwork class which trains the model by taking in the states and running the Model class to get the q values which are then indexed by the actions and then the gradients are calculated from the MSE loss between the predicted q value and the q_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, action_range):\n",
    "        super().__init__()\n",
    "        self.action_low, self.action_high = torch.from_numpy(np.array(action_range))\n",
    "        self.layer1 = nn.Linear(state_size, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.action = nn.Linear(100, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        layer1 = F.relu(self.layer1(state))\n",
    "        layer2 = F.relu(self.layer2(layer1))\n",
    "        layer3 = F.relu(self.layer3(layer1))\n",
    "        action = torch.sigmoid(self.action(layer3))\n",
    "        return self.action_low + (self.action_high-self.action_low)*action\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.net_state = nn.Linear(state_size, 100)\n",
    "        self.net_action = nn.Linear(action_size, 100)\n",
    "        self.net_layer = nn.Linear(200, 100)\n",
    "        self.q_value = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        net_state = F.relu(self.net_state(state))\n",
    "        net_action = F.relu(self.net_action(action))\n",
    "        net_state_action = torch.cat([net_state, net_action], dim=1)\n",
    "        net_layer = F.relu(self.net_layer(net_state_action))\n",
    "        q_value = self.q_value(net_layer)\n",
    "        return q_value\n",
    "\n",
    "class PTACNetwork():\n",
    "    def __init__(self, state_size, action_size, action_range): \n",
    "        self.actor_local = Actor(state_size, action_size, action_range)\n",
    "        self.actor_target = Actor(state_size, action_size, action_range)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=0.00025)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size)\n",
    "        self.critic_target = Critic(state_size, action_size)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=0.00025)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(np.array(state)).float()\n",
    "        action = self.actor_local(state).detach().numpy()\n",
    "        return action\n",
    "    \n",
    "    def update_model(self, state, action, next_state, reward, done, gamma=0.97):\n",
    "        states = torch.from_numpy(np.vstack(state)).float()\n",
    "        actions = torch.from_numpy(np.vstack(action)).float()\n",
    "        next_states = torch.from_numpy(np.vstack(next_state)).float()\n",
    "        rewards = torch.from_numpy(np.vstack(reward)).float()\n",
    "        dones = torch.from_numpy(np.vstack(done)).float()\n",
    "        next_actions = self.actor_local(next_states)\n",
    "        \n",
    "        q_targets = rewards + gamma * self.critic_target(next_states, next_actions).detach() * (1-dones)\n",
    "        critic_loss = F.mse_loss(self.critic_local(states, actions), q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        q_baseline = self.critic_local(states, actions).detach()\n",
    "        actor_gain = -(self.critic_local(states, self.actor_local(states)) - q_baseline)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_gain.mean().backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_copy(self.actor_local, self.actor_target)\n",
    "        self.soft_copy(self.critic_local, self.critic_target)\n",
    "        \n",
    "    def soft_copy(self, local, target, tau=0.01):\n",
    "        for t,l in zip(target.parameters(), local.parameters()):\n",
    "            t.data.copy_(t.data + tau*(l.data - t.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "Below is the implementation of a Replay Buffer using the deque collection as the rolling buffer of experience tuples. This can be sampled by specifying the sample size and then returns each individual experience type as separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYKcRk3ZqsMJ"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size)\n",
    "        return map(list, zip(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration Noise\n",
    "Below is the noise process class for exploration of continuous action spaces where noise applied to the action vector varies randomly with inertia from the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise():\n",
    "    def __init__(self, size, scale, mu=0.0, sigma=0.4, theta=0.15, decay=0.99):\n",
    "        self.noise = np.zeros(size)\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.decay = decay\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise = np.zeros(self.size)\n",
    "        self.scale *= self.decay\n",
    "        \n",
    "    def sample(self):\n",
    "        sample = self.theta * (self.mu - self.noise) + self.sigma * np.random.randn(self.size)\n",
    "        self.noise = sample * self.scale\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent\n",
    "Below is the implementation of the agent that uses Deep Actor Critic networks to learn the best action for a given state such that it achieves a reward is better than the critic's estimated value of it. It selects an action from the actor model which then explores by adding a noise process to it which decays over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKIXlcQSqsMN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFACNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8b7b72eaf2d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mActorCriticAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Initializing the agent and the model for selecting actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTFACNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# The number of state values in the state vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstate_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-8b7b72eaf2d2>\u001b[0m in \u001b[0;36mActorCriticAgent\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mActorCriticAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Initializing the agent and the model for selecting actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTFACNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;31m# The number of state values in the state vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstate_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFACNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "class ActorCriticAgent():\n",
    "    # Initializing the agent and the model for selecting actions\n",
    "    def __init__(self, env, network=TFACNetwork):\n",
    "        # The number of state values in the state vector\n",
    "        state_size = np.prod(env.observation_space.shape)\n",
    "        # The number of action indices to select from\n",
    "        action_size = np.prod(env.action_space.shape)\n",
    "        # The continuous range of the actions\n",
    "        action_range = [env.action_space.low, env.action_space.high]\n",
    "        # Defining the q network to use for modeling the Bellman equation\n",
    "        self.q_network = network(state_size, action_size, action_range)\n",
    "        # Defining the replay buffer for experience replay\n",
    "        self.replay_buffer = ReplayBuffer(100000)\n",
    "        # Initializing the epsilon value to 1.0 for initial exploration\n",
    "        self.noise_process = OUNoise(action_size, action_range[1]-action_range[0])\n",
    "        \n",
    "    # Function for getting an action to take in the given state\n",
    "    def get_action(self, state):\n",
    "        # Get the action from the network and add noise to it\n",
    "        return self.q_network.get_action([state])[0] + self.noise_process.sample()\n",
    "        \n",
    "    # Function for training the agent at each time step\n",
    "    def train(self, state, action, next_state, reward, done, batch_size=100):\n",
    "        # First add the experience to the replay buffer\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
    "        # Sample a batch of each experience type from the replay buffer\n",
    "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(batch_size)\n",
    "        # Train the model with the q target\n",
    "        self.q_network.update_model(states, actions, next_states, rewards, dones)\n",
    "        # Decrease epsilon after each episode\n",
    "        if done: self.noise_process.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Below is the training loop for training the agent through a number of episodes of interacting with the environment. It keeps track of the total reward from each episode and also stores the last 100 episode rewards for calculating the average reward for checking when the environment was solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3434
    },
    "colab_type": "code",
    "id": "zv9Amjj4qsMQ",
    "outputId": "92eeb722-4e00-4c1b-bd4d-63aa795954f1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ActorCriticAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fa48961ed4fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create an agent instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActorCriticAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPTACNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Define number of episodes to train for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create a buffer for calculating the last 100 episode average reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ActorCriticAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an agent instance\n",
    "agent = ActorCriticAgent(env, network=PTACNetwork)\n",
    "# Define number of episodes to train for\n",
    "num_episodes = 200\n",
    "# Create a buffer for calculating the last 100 episode average reward\n",
    "scores_buffer = deque(maxlen=100)\n",
    "# List to store each episode's total reward\n",
    "scores = []\n",
    "# List to store the average reward after each episode\n",
    "avg_scores = []\n",
    "\n",
    "# Run the training loop\n",
    "for ep in range(num_episodes):\n",
    "    # Save the initial state\n",
    "    state = env.reset()\n",
    "    # Reset the total reward\n",
    "    total_reward = 0\n",
    "    # Reset the episode terminal condition\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Query the agent for an action to take in the state\n",
    "        action = agent.get_action(state)\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Train the agent with the new time step experience\n",
    "        agent.train(state, action, next_state, reward, int(done))\n",
    "        # Update the episode's total reward\n",
    "        total_reward += reward             \n",
    "        # Update the current state\n",
    "        state = next_state \n",
    "\n",
    "    # Store the last episode's total reward\n",
    "    scores.append(total_reward)\n",
    "    # Add the total reward to the buffer for calculating average reward\n",
    "    scores_buffer.append(total_reward)\n",
    "    # Store the new average reward\n",
    "    avg_scores.append(np.mean(scores_buffer))\n",
    "    print(\"Episode: {}, Score: {}, Avg reward: {:.2f}\".format(ep, scores[ep], avg_scores[ep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU1OmrO0uHpO"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-731adf8177f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Scores\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Avg Score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = range(len(scores))\n",
    "plt.plot(x, scores, label=\"Scores\")\n",
    "plt.plot(x, avg_scores, label=\"Avg Score\")\n",
    "plt.title(\"Scores\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UBHQMHzqsMU"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DiscretizedDQNsForContinuousControl.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
